# -*- coding: utf-8 -*-
"""RLproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qgllht6oFva_PhkS6gzT0519fy8_Ljjz
"""

from scipy.stats import gamma
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import truncnorm
from scipy.optimize import minimize
import scipy.stats as stats
from scipy.stats import truncnorm
import numpy as np
from scipy.stats import gamma as gamma_dist
from scipy.stats import gamma as gamma_stats
import matplotlib.pyplot as plt

q = 0.1
def Rul(L, deg, alpha, scale):
    RUL = np.zeros(40)
    best = 0  # Initialize 'best' with a default value
    for i in range(40):

      RUL[i] = (1- q - gamma.cdf(L - deg, (i) * 1+0.1,1))
      if RUL[i] < 0:
        best = i

    return best

def truncnormal(delta_T):

  mu = delta_T / 2
  sigma = delta_T / 6
  r = truncnorm.rvs(0, delta_T, mu, scale=sigma, size=1)
  if(r<1):
    r=1
  return int(r)

Ci = 2
C0 = 20
S = 10
Cp = 120
Cc = 200
L = 40

# all of the states possible matrix
ALLSTATES = np.zeros((16, 3))
n = 0
for i in range(4):
    for j in range(2):
        for k in range(2):
            ALLSTATES[n] = [i,k, j]
            n += 1

ALLSTATES = np.vstack((ALLSTATES, [4, 0, 0]))

ALLSTATES

def rewardfunc():
  reward = np.zeros((17, 3))
  reward[:,1] = -S - Ci
  reward[:, 2] = -Ci
  for i in range(17):
    reward[i,0] = -Ci - Cp + C0 if ALLSTATES[i, 1] == 1 else -Ci - Cp
    reward[i,0]+= -28 if ALLSTATES[i, 2] == 0 else 0
  reward[16,0] = -Cc - Ci
  return reward

reward_matrix = rewardfunc()

reward_matrix

def encode_decode(input,EncodeFlage=0):
  if(EncodeFlage==0):
    if input == [0,0,0]:
      output = 0
    elif input == [0,1,0]:
      output =1
    elif input == [0,0,1]:
      output =2
    elif input == [0,1,1]:
      output =3
    elif input == [1,0,0]:
      output = 4
    elif input == [1,1,0]:
      output =5
    elif input == [1,0,1]:
      output =6
    elif input == [1,1,1]:
      output =7

    elif input == [2,0,0]:
      output = 8
    elif input == [2,1,0]:
      output =9
    elif input == [2,0,1]:
      output =10
    elif input == [2,1,1]:
      output =11

    elif input == [3,0,0]:
      output = 12
    elif input == [3,1,0]:
      output =13
    elif input == [3,0,1]:
      output =14
    elif input == [3,1,1]:
      output =15
    elif input ==[4,0,0]:
      output =16
  else:
    if input ==0:
      output = [0,0,0]
    elif input ==2:
      output = [0,0,1]
    elif input ==1:
      output = [0,1,0]
    elif input ==3:
      output = [0,1,1]

    elif input ==4:
      output = [1,0,0]
    elif input ==6:
      output = [1,0,1]
    elif input ==5:
      output = [1,1,0]
    elif input ==7:
      output = [1,1,1]

    elif input ==8:
      output = [2,0,0]
    elif input ==10:
      output = [2,0,1]
    elif input ==9:
      output = [2,1,0]
    elif input ==11:
      output = [2,1,1]

    elif input ==12:
      output = [3,0,0]
    elif input ==14:
      output = [3,0,1]
    elif input ==13:
      output = [3,1,0]
    elif input ==15:
      output = [3,1,1]
    elif input == 16:
      output = [4,0,0]
  return output

# StateNama : [deg , opp , resource ,delta_opp,delta_T]
def STATE(StateNama,primerly_Flag=1):
  if StateNama[1]==1:
    diff_T = StateNama[3]
  else:
    diff_T = StateNama[4]

  if primerly_Flag==1:
    X = gamma.rvs(diff_T, size=1)
    previous_deg = StateNama[0]
    deg= int(previous_deg + X) if (previous_deg + X)<L else L
    StateNama[0]=deg
  else:
    deg = StateNama[0]
  rul = Rul(L, deg, 1, 1)

  delta_T = StateNama[4]
  delta_opp = StateNama[3]
  if deg >= L:
    state = [4, 0, 0]
  elif  rul >= 2 * delta_T:
    state = [0, StateNama[1], StateNama[2]]
  elif  rul >= delta_T and rul < 2 * delta_T:
    state = [1, StateNama[1], StateNama[2]]
  elif  rul <= delta_T and rul > delta_opp:
    state = [2, StateNama[1], StateNama[2]]
  elif  rul <= delta_opp:
    state = [3, StateNama[1], StateNama[2]]
    # Add a default return statement to handle any unexpected cases
  else:
      state = [0, 0, 0]
  return state,StateNama

# StateNama : [deg , opp , resource ,delta_opp,delta_T]
def ENVIRONMENT( action, StateNama,current_state):

  next_state = [0,0,0]
  index_current_state = encode_decode(current_state)
  reward_matrix = rewardfunc()
  reward = reward_matrix[ index_current_state,action]
  p_opp = np.random.rand()
  delta_opp = 0
  opp = 0
  if p_opp > 0.5:
    opp = 1
    delta_opp = truncnormal(delta_T)

  if action == 0:
    StateNama[0] = 0
    StateNama[1] = 0
    StateNama[2] = 0
    StateNama[3] = 0

  elif action == 1:
    StateNama[1] = opp
    StateNama[2] = 1
    StateNama[3] = delta_opp

  elif action == 2:
    StateNama[1] = opp
    StateNama[3] = delta_opp

  else:
    print("Wrong Action",action)
    next_state = [100,100,100]

  next_state,StateNama = STATE(StateNama,0)

  return next_state,StateNama, reward

delta_T = 10
StateNama = [0,0,0,0,10]
for _ in range(20):
  # StateNama : [deg , opp , resource ,delta_opp,delta_T]

  if StateNama[0]>L-1:
    action = 0
  else:
    action = np.random.randint(1,3)
  current_state,StateNama = STATE(StateNama)
  print( action, StateNama,current_state)
  next_state,StateNama,reward = ENVIRONMENT( action, StateNama,current_state)
  print("current_state:", current_state)
  print("nextState:", next_state)
  print("Opp:", StateNama[1])
  print("Delta Opp:", StateNama[3])
  print("final Deg:", StateNama[0])
  print("Action:", action)
  print("Reward:", reward)
  print("Delta T:", StateNama[4])
  print("------------")

Q_table = np.zeros([17,3])
Q_table[ALLSTATES[:,1]==1,1]=-1000
Q_table[16,[1,2]]=-1000

Q_table

import numpy as np
import matplotlib.pyplot as plt

# Hyperparameters
  # Learning rate
delta_T = 5
q =0.1
alpha = 1 # learning rate
max_alpha = 1
min_alpha = 0
alpha_decay_rate = 0.01
discount_factor = 0.85  # Discount factor
num_episodes = 500 # Number of episodes

epsilon = 1  # Epsilon-greedy exploration rate
max_epsilon = 1
min_epsilon = 0.01
exploration_decay_rate = 0.01

max_step = 50
# Initialize Q-table with small random values
num_states = 17
num_actions = 3
#Q_table = np.random.uniform(low=-0.1, high=0, size=(num_states, num_actions))
Q_table = np.zeros([num_states,num_actions])
Q_table[ALLSTATES[:,2]==1,1]=-1000
Q_table[16,[1,2]]=-1000
# Create a single random number generator instance
rng = np.random.default_rng()

# Q-learning algorithm
episode_list = []
reward_list = []
step = 0
CC = 0
duration = 0
for episode in range(num_episodes):
    StateNama = [0, 0, 0, 0, delta_T]
    done = False
    total_reward = 0
    Current_State, StateNama = STATE(StateNama, 1)
    index_state = encode_decode(Current_State)
    opp_counter = 0
    duration = 0

    while not done:
        step +=1
        if rng.uniform() < epsilon:
            action = rng.integers(num_actions)
            while (Q_table[index_state,action] == -1000) :
              action = rng.integers(num_actions)

        else:
            action = np.argmax(Q_table[index_state])
        if(StateNama[1]==1):
          duration +=StateNama[3]
        else:
          duration +=StateNama[4]

        next_state, StateNama, reward_value = ENVIRONMENT(action, StateNama, Current_State)


        Ayande_state,StateNama = STATE(StateNama, 1)
        index_Ayande_state = encode_decode(Ayande_state)
        # Update Q-table using the Q-learning equation
        max_next_q = np.max(Q_table[index_Ayande_state])
        Q_table[index_state, action] += alpha * (
            reward_value + discount_factor * max_next_q - Q_table[index_state, action]
        )

        total_reward += reward_value

        Current_State= Ayande_state

        index_state = encode_decode(Current_State)

        if step == max_step or episode == num_episodes  :
            done = True
    # Exploration rate decay
    print(duration)
    epsilon = min_epsilon + \
    (max_epsilon - min_epsilon) * np.exp(-exploration_decay_rate*episode)
    alpha = min_alpha + \
    (max_alpha - min_alpha) * np.exp(-alpha_decay_rate*episode)
    print(total_reward/duration)


    reward_list.append(total_reward)
    episode_list.append(episode)
    step = 0
    print("Episode:", episode + 1)
    print("Total Reward:", total_reward)
    print("Q-table:")
    print(Q_table)
    print("--------")

# Calculate and print the average reward per episode
average_reward = np.mean(reward_list)
print("Average Reward per Episode:", average_reward)

# Plot episode rewards
def plot_rewards(episode_list, reward_list):
    plt.plot(episode_list, reward_list)
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("Q-Learning Episode Rewards")
    plt.show()

# Call the plot_rewards function
plot_rewards(episode_list, reward_list)

gamma.cdf(23,14,1)

gamma.rvs(18,1)

q = 0.06
Rul(L, 0, 1, 1)

max_alpha = 1
min_alpha = 0.001
alpha_decay_rate = 0.01
discount_factor = 0.85  # Discount factor
num_episodes = 250 # Number of episodes
al = []
for i in range(num_episodes):

  alpha = min_alpha + \
  (max_alpha - min_alpha) * np.exp(-alpha_decay_rate*i)
  al.append(alpha)

plt.plot(al)
plt.xlabel("  Episodes")
plt.ylabel("Learning rate")

max_epsilon = 1
min_epsilon = 0.000
epsilon_decay_rate = 0.01
discount_factor = 0.85  # Discount factor
num_episodes = 250 # Number of episodes
al = []
for i in range(num_episodes):

  epsilon = min_alpha + \
  (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*i)
  al.append(epsilon)

plt.plot(al)
plt.xlabel("  Episodes")
plt.ylabel("Epsilon")

